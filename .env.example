# LLM Provider Configuration
# Choose your LLM backend: openai, ollama, anthropic
LLM_BACKEND=openai

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here

# Ollama Configuration (for local LLMs)
OLLAMA_URL=http://localhost:11434/api/chat

# Anthropic Configuration (if you add anthropic_provider.py)
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Cache Configuration
# Directory for caching LLM responses
LLM_CACHE_DIR=.llm_cache

# HuggingFace Configuration (for RAG service)
HUGGINGFACE_AUTH_TOKEN=your-huggingface-token-here

# Other service configurations can be added below