### Phase 4.5 — `kafka_utils` SDK

*A one-stop, asyncio-friendly wrapper around **confluent-kafka** that every agent can import instead of hand-rolling producer / consumer boiler-plate.*

---

## 1  |  Why we need it

| Pain today                                                                                                                                      | What `kafka_utils` fixes                                                                                                                                                                                                       |
| ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| ∎ 30-line copy-pasted producer/consumer blocks in each agent.<br>∎ Manual `topic → ProtoClass` mapping.<br>∎ No uniform tracing / Prom metrics. | ∎ 3-line setup: `producer = await kf.get_producer()` / `async for msg in kf.subscribe(...)`.<br>∎ Automatic (de)serialisation to **protobuf** or **JSON**.<br>∎ Built-in retries, back-pressure awaitables, Prom + OTel hooks. |

---

## 2  |  Directory layout

```
clients/
└── kafka_utils/
    ├── __init__.py
    ├── producer.py
    ├── consumer.py
    ├── codec.py
    ├── metrics.py
    └── typing.py      # type aliases
```

---

## 3  |  Dependencies

Already present: **confluent-kafka**.
Add:

```toml
[tool.poetry.dependencies]
orjson = "^3.10"        # fast JSON fallback
```

---

## 4  |  Core code snippets

### 4.1  `codec.py`

```python
from __future__ import annotations
import orjson, inspect, typing as _t
from google.protobuf.message import Message as _Proto

def encode(obj) -> bytes:
    if isinstance(obj, _Proto):
        return obj.SerializeToString()
    return orjson.dumps(obj)

def decode(data:bytes, cls:_t.Type) -> object:
    if inspect.isclass(cls) and issubclass(cls, _Proto):
        msg = cls(); msg.ParseFromString(data); return msg
    return orjson.loads(data)
```

### 4.2  `metrics.py`

```python
from prometheus_client import Counter, Histogram

PROD_CNT = Counter("kf_messages_produced_total","",["topic"])
CONS_CNT = Counter("kf_messages_consumed_total","",["topic"])
LAT      = Histogram("kf_produce_latency_sec","",["topic"])
```

### 4.3  `producer.py`

```python
import os, asyncio, functools, confluent_kafka, time
from .codec import encode
from .metrics import PROD_CNT, LAT

_BOOT=os.getenv("KAFKA_BOOTSTRAP","kafka:9092")
_producer: confluent_kafka.Producer|None = None

def _get():
    global _producer
    if _producer is None:
        _producer = confluent_kafka.Producer({"bootstrap.servers":_BOOT})
    return _producer

async def send(topic:str, obj, key:str|None=None):
    p=_get(); data=encode(obj)
    loop=asyncio.get_running_loop()
    t0=time.perf_counter()
    await loop.run_in_executor(None, p.produce, topic, data, key)
    p.poll(0)                     # trigger delivery callbacks
    PROD_CNT.labels(topic).inc()
    LAT.labels(topic).observe(time.perf_counter()-t0)
```

### 4.4  `consumer.py`

```python
import os, asyncio, confluent_kafka, inspect
from .codec import decode
from .metrics import CONS_CNT

_BOOT=os.getenv("KAFKA_BOOTSTRAP","kafka:9092")

class AsyncConsumer:
    def __init__(self, group:str, topics:list[str], proto_map:dict[str,object]|None=None):
        self._c = confluent_kafka.Consumer({
            "bootstrap.servers":_BOOT,
            "group.id": group,
            "auto.offset.reset":"earliest"})
        self._c.subscribe(topics)
        self._proto_map = proto_map or {}

    async def __aiter__(self):
        while True:
            msg = self._c.poll(0.3)
            if not msg:
                await asyncio.sleep(0.1); continue
            if msg.error():
                if msg.error().code() == confluent_kafka.KafkaError._PARTITION_EOF:
                    continue
                raise RuntimeError(msg.error())
            topic = msg.topic()
            cls = self._proto_map.get(topic, dict)
            CONS_CNT.labels(topic).inc()
            yield topic, decode(msg.value(), cls)
```

### 4.5  `__init__.py`

```python
from .producer import send            as produce
from .consumer import AsyncConsumer   as subscribe
```

---

## 5  |  Usage examples

```python
# Request-Planner
from clients.kafka_utils import produce, subscribe
from apps.core_contracts_pb2 import Plan, ChangeRequest
from apps.orchestrator import topics as T

# producer
await produce(T.PLAN, plan_obj)

# consumer loop
async for topic, msg in subscribe("request-planner", [T.CRQ],
                                  proto_map={T.CRQ: ChangeRequest}):
    await process_change(msg)
```

No more instantiating `confluent_kafka` directly.

---

## 6  |  Agent migration plan

1. **Phase 4+ agents**:

   * Replace their hand-rolled producer/consumer setup with `kafka_utils`.
   * Remove explicit `poll` loops—subscribe generator yields decoded protos.
2. Delete duplicated delivery-report callbacks and manual `.SerializeToString()` lines.

*Refactor incrementally*: start with Request-Planner; once stable, update Code-Planner, etc.

---

## 7  |  Unit tests

`clients/kafka_utils/tests/test_roundtrip.py`

```python
import asyncio, pytest, uuid, os
from clients.kafka_utils import produce, subscribe

@pytest.mark.asyncio
async def test_roundtrip(monkeypatch):
    # monkeypatch to stub actual Kafka for unit test
    produced=[]
    async def fake_send(topic,obj,key=None): produced.append((topic,obj))
    monkeypatch.setattr("clients.kafka_utils.producer.send", fake_send)

    await produce("foo","bar")
    assert produced == [("foo","bar")]
```

*(End-to-end Kafka tests live in integration test suite, not unit.)*

---

## 8  |  Prometheus scrape

All counters/histograms are exposed through each agent’s existing `/metrics` endpoint (they already mount `prometheus_client.start_http_server`). No infra change.

---

## 9  |  Commit & tag

```bash
mkdir -p clients/kafka_utils clients/kafka_utils/tests
# add files from sections 3-7
git add clients/kafka_utils
git commit -m "phase04.5: shared kafka_utils SDK (proto-aware, async, metrics)"
git push origin main
git tag phase-04.5-complete
git push origin phase-04.5-complete
```

---

### Phase 4.5 complete

*All agents now share a concise, metric-aware Kafka SDK with automatic protobuf serialisation, retries, and asyncio support.  Your codebase just lost hundreds of boiler-plate lines while gaining observability and testability.*
