### Phase 04 — Request-Planner Agent

*Goal: create a **stateless micro-service** that listens for `ChangeRequest` messages, queries SRM + RAG, and emits a structured `Plan` proto to Kafka so the Orchestrator can continue to the code phase.*

---

## 0  |  New/Updated Dependencies

Add to **`pyproject.toml`**

```toml
[tool.poetry.dependencies]
openai          = "^1.15"     # LLM call
tiktoken        = "^0.6"
anyio           = "^4.4"
```

Install:

```bash
poetry install
```

*(OpenAI key via `OPENAI_API_KEY` env var; in CI we’ll stub the call.)*

---

## 1  |  Extend Protobuf

Append to **`proto/core_contracts.proto`**:

```proto
message Step {
  int32  order   = 1;
  string goal    = 2;
  string kind    = 3;          // ADD | EDIT | REMOVE | REFACTOR
  string path    = 4;          // file to touch
}

message Plan {
  string id              = 1;
  string parent_request_id = 2;
  repeated Step steps     = 3;
  repeated string rationale = 4;
  repeated string reserved_lease_ids = 5;
}
```

Regenerate stubs:

```bash
python -m grpc_tools.protoc -I proto --python_out=apps proto/core_contracts.proto
```

---

## 2  |  Topics update

Add to **`apps/orchestrator/topics.py`** (only constants—Orchestrator already consumes `PLAN`):

```python
PLAN = "plan.out"           # emitted by Request-Planner
```

No orchestrator change needed; it already waits on messages on `plan.out`.

---

## 3  |  Request-Planner service code

`apps/agents/request_planner/agent.py`

```python
import os, uuid, json, re, asyncio, logging, anyio
from datetime import datetime
from confluent_kafka import Producer, Consumer, KafkaError
from apps.core_contracts_pb2 import ChangeRequest, Plan, Step
from clients.srm_client import SRMClient
from clients.rag_client import RagClient        # to be created in Phase 05
from .prompt import build_prompt, parse_json    # in step-4
from apps.orchestrator import topics as T

BOOT = os.getenv("KAFKA_BOOTSTRAP","kafka:9092")
GROUP = "request-planner"

log = logging.getLogger("request-planner")

producer = Producer({"bootstrap.servers": BOOT})
consumer = Consumer({
    "bootstrap.servers": BOOT,
    "group.id": GROUP,
    "auto.offset.reset": "earliest"
})
consumer.subscribe([T.CRQ])

srm = SRMClient("srm", 9090)
rag = RagClient("rag_service", 9100)

RESERVE_RE = re.compile(r"\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\(")  # simple fn capture

async def process_change(msg):
    cr = ChangeRequest.FromString(msg.value())
    ctx_snips = await rag.hybrid_search(cr.description, k=8, alpha=.3)
    prompt = build_prompt(cr, ctx_snips)
    llm_json = await parse_json(prompt)
    plan = Plan(
        id=str(uuid.uuid4()),
        parent_request_id=cr.id,
        rationale=llm_json["rationale"]
    )
    # steps
    for i, step_js in enumerate(llm_json["steps"], 1):
        plan.steps.append(
            Step(order=i, goal=step_js["goal"],
                 kind=step_js["kind"], path=step_js.get("path",""))
        )
    # reserve symbol names
    for sym in RESERVE_RE.findall(cr.description):
        try:
            reply = await srm.reserve(
                repo=cr.repo, branch=cr.branch,
                fq_name=sym, kind="function",
                file_path=step_js.get("path",""), plan_id=plan.id, ttl_sec=600)
            plan.reserved_lease_ids.append(str(reply.lease_id))
        except Exception as e:
            log.warning("Symbol conflict for %s: %s", sym, e)
    producer.produce(T.PLAN, plan.SerializeToString())
    producer.flush()

async def main_loop():
    while True:
        msg = consumer.poll(0.3)
        if not msg:
            await asyncio.sleep(0.1); continue
        if msg.error() and msg.error().code() != KafkaError._PARTITION_EOF:
            log.error("Kafka error %s", msg.error()); continue
        await process_change(msg)

if __name__ == "__main__":
    asyncio.run(main_loop())
```

---

## 4  |  Prompt helper

`apps/agents/request_planner/prompt.py`

```python
import os, json, openai, asyncio, tiktoken

SYSTEM = """You are Request-Planner v1.
Return ONLY valid JSON with keys:
  steps: [{goal, kind, path}], rationale: [...]"""

async def parse_json(prompt):
    resp = await openai.ChatCompletion.acreate(
        model=os.getenv("PLANNER_MODEL","gpt-4o-mini"),
        temperature=0.1,
        messages=[{"role":"system","content":SYSTEM},
                  {"role":"user","content":prompt}],
        response_format={"type":"json_object"}
    )
    return json.loads(resp.choices[0].message.content)

def build_prompt(cr, snippets):
    ctx = "\n\n".join(snippets)
    return f"""# CHANGE REQUEST
{cr.description}

# CONTEXT
{ctx}

Return plan JSON."""
```

*(For CI we’ll monkey-patch `openai` to return fixed JSON.)*

---

## 5  |  Dockerfile

`apps/agents/request_planner/Dockerfile`

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY pyproject.toml poetry.lock* /app/
RUN pip install --no-cache-dir poetry && \
    poetry export -f requirements.txt --without-hashes | pip install -r /dev/stdin
COPY apps/agents/request_planner /app/apps/agents/request_planner
COPY clients /app/clients
CMD ["python", "-m", "apps.agents.request_planner.agent"]
```

Extend **`docker-compose.yml`**

```yaml
  request_planner:
    build: ./apps/agents/request_planner
    environment:
      KAFKA_BOOTSTRAP: kafka:9092
      OPENAI_API_KEY: "sk-dummy"     # will be overridden in prod
    depends_on: [kafka, orchestrator, srm, rag_service]
```

---

## 6  |  Unit tests (offline)

`apps/agents/request_planner/tests/test_planner.py`

```python
import pytest, json, asyncio
from apps.agents.request_planner import prompt

class Dummy:
    async def acreate(*_, **__):
        class R:
            choices=[type("x",(object,),{"message":type("y",(object,),{"content":json.dumps({
                "steps":[{"goal":"add greet()","kind":"ADD","path":"src/app.py"}],
                          "rationale":["simple feature"]})})()})]
        return R()
prompt.openai.ChatCompletion = Dummy   # patch

@pytest.mark.asyncio
async def test_parse_json():
    out = await prompt.parse_json("hi")
    assert out["steps"][0]["goal"] == "add greet()"
```

---

## 7  |  CI update

Add to workflow before lint step:

```yaml
      - name: Run planner unit-tests
        run: poetry run pytest apps/agents/request_planner/tests -q
```

No external services required (openai patched).

---

## 8  |  Smoke test end-to-end (local)

```bash
docker compose up --build -d kafka zookeeper postgres qdrant \
                  srm rag_service orchestrator request_planner
python scripts/mock_change_request.py   # from Phase-03
docker-compose logs -f orchestrator | grep "PLAN"   # should see transition
```

---

## 9  |  Prometheus counter

Inside `agent.py`, increment:

```python
from prometheus_client import Counter, start_http_server
PLANS = Counter("rp_plans_total","plans emitted")
start_http_server(9400)
...
producer.produce(T.PLAN, plan.SerializeToString())
PLANS.inc()
```

---

## 10  |  Commit & tag

```bash
git add proto/core_contracts.proto apps/agents/request_planner docker-compose.yml
git commit -m "phase04: request-planner agent w/ SRM + RAG + OpenAI"
git push origin main
git tag phase-04-complete
git push origin phase-04-complete
```

---

### Phase 04 complete

*Request-Planner is live: on every `ChangeRequest` it reserves symbols via SRM, consults RAG for context, generates a structured Plan with GPT-4o-mini, and publishes it to `plan.out`. Orchestrator now advances automatically to the code phase.*

Proceed to **Phase 05 — Code-Planner Agent**.
