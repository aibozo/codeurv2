### Phase 06 — Coding-Agent

*Goal: create a worker that consumes each `CodingTask`, generates a patch with an LLM, lint-/test-checks it locally, commits to a feature branch, claims reserved symbols in SRM, then emits a `CommitResult` to Kafka.*

---

## 0  |  Prerequisites & New Libraries

| Library                     | Why                          | Install                |
| --------------------------- | ---------------------------- | ---------------------- |
| `unidiff` 0.7               | parse/validate unified diffs | `poetry add unidiff`   |
| `gitpython` 3.2             | local commit & branch ops    | `poetry add gitpython` |
| `pytest` & `pytest-testmon` | fast selective tests         | already dev-deps       |
| `black`, `ruff`             | formatting / linting         | already dev-deps       |
| `openai`                    | LLM diff generation          | added in Phase 04      |

Add to **`pyproject.toml`** if not present.

---

## 1  |  Protobuf recap (no change)

Coding-Agent receives `CodingTask` (Phase 05) and publishes `CommitResult`.

```proto
message CommitResult {
  string task_id     = 1;
  string commit_sha  = 2;
  string status      = 3;   // SUCCESS | SOFT_FAIL | HARD_FAIL
  string branch_name = 4;
  repeated string notes = 5;
}
```

---

## 2  |  Agent design

```
CodingAgent (K8s Job, one per task)
   │
   ├─ pull repo@base_commit (shallow)
   ├─ RAG.snippet(blob_ids)  → build LLM prompt
   ├─ openai.chat: returns {"diff": "...", "reasoning":"..."}
   ├─ validate & git apply
   ├─ self-check pipeline:
   │     black → ruff → pytest -q -m fast
   │
   ├─ if pass:
   │     git commit -m "feat(task_id): goal"
   │     git push origin agt/<task_id>
   │     SRM.claim(lease_ids)
   │     emit CommitResult SUCCESS
   └─ else:
         (retry ≤ 2) OR emit CommitResult SOFT_FAIL/HARD_FAIL
```

---

## 3  |  Service code

`apps/agents/coding_agent/agent.py`

```python
import os, asyncio, subprocess, tempfile, uuid, json, logging, shutil, difflib
from pathlib import Path
from confluent_kafka import Consumer, Producer, KafkaError
from unidiff import PatchSet
from git import Repo, GitCommandError
from apps.core_contracts_pb2 import CodingTask, CommitResult
from clients.rag_client import RagClient
from clients.srm_client import SRMClient
from apps.orchestrator import topics as T
import openai, anyio

BOOT = os.getenv("KAFKA_BOOTSTRAP","kafka:9092")
consumer = Consumer({"bootstrap.servers":BOOT,
                     "group.id":"coding-agent","auto.offset.reset":"earliest"})
consumer.subscribe([T.TASK])
producer = Producer({"bootstrap.servers":BOOT})
rag = RagClient("rag_service",9100)
srm = SRMClient("srm",9090)
log = logging.getLogger("coding-agent")

LLM_MODEL = os.getenv("CODING_MODEL","gpt-4o-mini-code-30k")
MAX_RETRIES = 2

async def llm_patch(task:CodingTask, ctx_text:str)->dict:
    system = "You are Coding-Agent. Return JSON {diff, reasoning}"
    user = f"""TASK GOAL:
{task.goal}

FILE PATH:
{task.path or 'N/A'}

CONTEXT (read-only):
{ctx_text[:3000]}"""
    resp = await openai.ChatCompletion.acreate(
        model=LLM_MODEL, temperature=0.1,
        messages=[{"role":"system","content":system},
                  {"role":"user","content":user}],
        response_format={"type":"json_object"})
    return json.loads(resp.choices[0].message.content)

def apply_patch(repo:Repo, diff:str)->bool:
    try:
        PatchSet(diff)       # validate diff syntax
        (repo.working_dir and
         subprocess.run(["git","apply","-"], input=diff.encode(),
                        cwd=repo.working_dir, check=True))
        return True
    except Exception as e:
        log.warning("patch reject %s", e); return False

def run_selfcheck(repo_dir:str)->tuple[bool,list]:
    notes=[]
    def _run(cmd):
        r = subprocess.run(cmd, cwd=repo_dir, capture_output=True, text=True)
        if r.returncode!=0: notes.append(r.stdout+r.stderr)
        return r.returncode==0
    ok = _run(["black","--check","."]) and \
         _run(["ruff","."]) and \
         _run(["pytest","-q","-m","fast"])
    return ok, notes

async def process(task:CodingTask):
    workdir = Path(tempfile.mkdtemp())
    repo = Repo.clone_from(os.getenv("REMOTE_REPO"), workdir, depth=1, branch=task.path.split("/")[0] if task.path else "main")
    ctx = "\n".join([c.snippet async for c in rag.snippet_stream(task.blob_ids)])
    for attempt in range(MAX_RETRIES+1):
        patch_json = await llm_patch(task, ctx)
        if not apply_patch(repo, patch_json["diff"]):
            continue
        ok, notes = run_selfcheck(repo.working_dir)
        if ok:
            branch=f"agt/{task.id}"
            repo.git.checkout("-b",branch)
            repo.git.add(all=True)
            repo.git.commit("-m",f"{task.kind.lower()}: {task.goal}\n\n[agent:{task.id}]")
            repo.git.push("origin",branch)
            for lid in task.reserved_lease_ids:
                try: await srm.claim(lease_id=int(lid), commit_sha=repo.head.commit.hexsha)
                except Exception: pass
            res=CommitResult(task_id=task.id, commit_sha=repo.head.commit.hexsha,
                             status="SUCCESS", branch_name=branch)
            producer.produce(T.CRES, res.SerializeToString()); producer.flush()
            shutil.rmtree(workdir); return
        if attempt<MAX_RETRIES:
            ctx += "\n\n# SELF-CHECK FAILURES\n" + "\n".join(notes)
    # hard fail
    res=CommitResult(task_id=task.id, commit_sha="", status="SOFT_FAIL", notes=notes)
    producer.produce(T.CRES,res.SerializeToString()); producer.flush()
    shutil.rmtree(workdir)

async def main_loop():
    while True:
        msg = consumer.poll(0.3)
        if not msg: await asyncio.sleep(0.1); continue
        if msg.error() and msg.error().code()!=KafkaError._PARTITION_EOF:
            log.error(msg.error()); continue
        tb = T.TaskBundle.FromString(msg.value())
        for task in tb.tasks:
            await process(task)

if __name__ == "__main__":
    asyncio.run(main_loop())
```

*(simplified: uses env `REMOTE_REPO` pointing to git server accessible from container)*

---

## 4  |  Dockerfile

`apps/agents/coding_agent/Dockerfile`

```dockerfile
FROM python:3.11-slim
RUN apt-get update && apt-get install -y git gcc && rm -rf /var/lib/apt/lists/*
WORKDIR /app
COPY pyproject.toml poetry.lock* /app/
RUN pip install poetry && poetry export -f requirements.txt --without-hashes | pip install -r /dev/stdin
COPY apps/agents/coding_agent /app/apps/agents/coding_agent
COPY clients /app/clients
CMD ["python", "-m", "apps.agents.coding_agent.agent"]
```

Extend **`docker-compose.yml`**

```yaml
  coding_agent:
    build: ./apps/agents/coding_agent
    environment:
      KAFKA_BOOTSTRAP: kafka:9092
      OPENAI_API_KEY: "sk-dummy"
      REMOTE_REPO: "https://github.com/your-org/self-healing-code"
    depends_on: [kafka, orchestrator, rag_service, srm]
    deploy:
      replicas: 3          # auto-parallel
```

---

## 5  |  Unit tests (mocked)

`apps/agents/coding_agent/tests/test_llm_patch.py`

```python
import asyncio, json
from apps.agents.coding_agent.agent import llm_patch
class Dummy:
    async def acreate(*_, **__):
        class R: choices=[type("x",(object,),{"message":type("y",(object,),{"content":json.dumps({
            "diff":"--- a/x\n+++ b/x\n","reasoning":"ok"})})()})]
        return R()
import apps.agents.coding_agent.agent as mod
mod.openai.ChatCompletion = Dummy
from apps.core_contracts_pb2 import CodingTask
t=CodingTask(goal="add greet()",path="dummy.py")
asyncio.run(llm_patch(t,"ctx"))
```

Add to CI.

---

## 6  |  Prometheus

Inside `agent.py`:

```python
from prometheus_client import Counter, start_http_server
PATCH_GEN = Counter("ca_patches_total","generated",["result"])
start_http_server(9600)
...
if ok: PATCH_GEN.labels("success").inc()
else:  PATCH_GEN.labels("fail").inc()
```

---

## 7  |  Smoke test (stub LLM)

For local demo, patch `openai.ChatCompletion` to return a trivial diff adding a newline to README. Or run agent with env `MOCK_LLM=1` to skip openai call (guard in code).

Run:

```bash
docker compose up --build -d kafka zookeeper postgres qdrant srm rag_service \
   orchestrator request_planner code_planner coding_agent
python scripts/mock_change_request.py
docker-compose logs -f orchestrator | grep BUILD1
```

You should see Orchestrator shift from `code_phase` to `build1` once tasks succeed.

---

## 8  |  Commit & tag

```bash
git add apps/agents/coding_agent docker-compose.yml
git commit -m "phase06: coding-agent generates and self-checks patches"
git push origin main
git tag phase-06-complete
git push origin phase-06-complete
```

---

### Phase 06 complete

*Coding-Agent farm now picks up every `CodingTask`, drafts a patch with the LLM and RAG context, runs format/lint/fast tests locally, commits to a feature branch, claims SRM leases, and emits `CommitResult`. The Orchestrator automatically advances to the first Build/CI stage.*
